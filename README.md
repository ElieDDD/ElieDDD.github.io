
# ElieDDD.github.io

'Datasets such as ImageNet are built on an array of practices of mediation of photography: collecting, labelling, composing, assembling images and distributing them.'
'In visual datasets, photographs are considered as self-standing documents free from the contexts from which they originated and through which they travelled.'
'The work of manually cross-referencing and labelling the photos is what makes datasets like ImageNet so unique[3]. In fact, there has been rarely in the history so many people paid to look at images and report what they see in them (Krishna et al, 2016). The automation of vision has not reduced but increased the number of eyeballs looking at images, of hands typing descriptions, of taggers and annotators. Yet what has changed is the context in which the activity of seeing is taking place, how retinas are entangled in heavily technical environments and how vision is driven by an extraordinary speed.'
https://unthinking.photography/articles/an-introduction-to-image-datasets

>Exploring Transformers, Embedding and other key aspects of Generative AI
>Computer Vision: image classification, object detection, and segmentation,  'Multimodal' zero-shot image classification. CNNs learning directly from images, pooling.
>See: https://huggingface.co/docs/transformers.js/en/index
'We find that use of model-generated content in training causes
 irreversible defects in the resulting models, where tails of the original content distribution disappear.
 We refer to this effect as model collapse1 and show that it can occur in Variational Autoencoders,
 Gaussian Mixture Models and LLMs. We build theoretical intuition behind the phenomenon and
 portray its ubiquity amongst all learned generative models'
>Shumailov, I., Shumaylov, Z., Zhao, Y., Gal, Y., Papernot, N., & Anderson, R. (2023). The Curse of Recursion: Training on Generated Data Makes Models Forget. ArXiv, abs/2305.17493.
>
>CNNs 'You train a CNN to do image analysis tasks, including scene classification, object detection and segmentation, and image processing. In order to understand how CNNs work, we'll cover three key concepts: local receptive fields, shared weights and biases, and activation and pooling.' https://uk.mathworks.com/

<b>'Unnatural learning procesess' </b>
Ideas: 'Feeding ordered data to a model sequentially can lead to an unnatural, unbalanced learning process.' Nguyen

'Augmentation helps generate more unbiased data' - such as flipping and rotating synthetic images and more to prevent <b> overfit </b>, look at Data Augmentation, see:https://github.com/aleju/imgaug

<B> INCEPTION </b> 
An Inception Module is an image model block that aims to approximate an optimal local sparse structure in a CNN. Put simply, it allows for us to use multiple types of filter size, instead of being restricted to a single filter size, in a single image block, which we then concatenate and pass onto the next layer.

102 Category Flower Dataset & many other image data sets:https://paperswithcode.com/datasets?mod=images

Hu, G., Peng, X., Yang, Y., Hospedales, T. M. and Verbeek, J. (2016) Frankenstein: Learning Deep Face Representations using Small Data, CoRR, abs/1603.0. Available from: http://arxiv.org/abs/1603.06470 [Accessed 29 September 2017].

