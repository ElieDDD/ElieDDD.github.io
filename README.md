# ElieDDD.github.io
>Exploring Transformers, Embedding and other key aspects of Generative AI
>Computer Vision: image classification, object detection, and segmentation,  'Multimodal' zero-shot image classification. CNNs learning directly from images, pooling.
>See: https://huggingface.co/docs/transformers.js/en/index
'We find that use of model-generated content in training causes
 irreversible defects in the resulting models, where tails of the original content distribution disappear.
 We refer to this effect as model collapse1 and show that it can occur in Variational Autoencoders,
 Gaussian Mixture Models and LLMs. We build theoretical intuition behind the phenomenon and
 portray its ubiquity amongst all learned generative models'
>Shumailov, I., Shumaylov, Z., Zhao, Y., Gal, Y., Papernot, N., & Anderson, R. (2023). The Curse of Recursion: Training on Generated Data Makes Models Forget. ArXiv, abs/2305.17493.
